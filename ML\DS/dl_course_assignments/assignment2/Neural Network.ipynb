{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QO4EvjkVzIix"
   },
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1594981904796,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "oAVhc2UVzIi6",
    "outputId": "d63daeab-a3bf-45c4-dee6-28c5b3681cdb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1404,
     "status": "ok",
     "timestamp": 1594981905414,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "WrWdMppBzIjQ"
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gBxlwctzIjb"
   },
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8435,
     "status": "ok",
     "timestamp": 1594969893151,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": -180
    },
    "id": "k75x5B8lzIjd"
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLLd2FLzzIjp"
   },
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1594974171274,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "cYC7H5JHzIjr",
    "outputId": "cfc56417-5baf-40c0-e0cc-37e3b0571a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iM8JGepjzIj1"
   },
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1594975885964,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "xi5PP4CHzIj3",
    "outputId": "3740df65-49e7-4835-d78a-0bc825a41591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RXM-11dzIkD"
   },
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6288,
     "status": "ok",
     "timestamp": 1594982982327,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "hihU0DBTzIkF",
    "outputId": "18d6b9c4-c40b-4603-d547-99842289129e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtippxopzIkR"
   },
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRnjw2TkzIkS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTCQW2W6zIkg"
   },
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1594984007940,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "T3Pp9N7ozIkj",
    "outputId": "418d5866-b4f8-4527-e0a2-60ce60d9a481"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJFu2JGCzIkt"
   },
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79002,
     "status": "ok",
     "timestamp": 1594985541953,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "WvAvE3rVzIkv",
    "outputId": "102a0c01-832a-43ea-c911-d3e7f3364e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.293779, Train accuracy: 0.194556, val accuracy: 0.206000\n",
      "Loss: 2.244658, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234127, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228683, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228387, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228343, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228290, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228298, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228295, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228292, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228286, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228294, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228298, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1594985450184,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "DIPrByO8zIk4",
    "outputId": "95e66481-838e-4258-c073-87ec4907552a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC7wRoZTzIla"
   },
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtVbbRwkzIlc"
   },
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1594985628953,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "lK-8qS0yzIle",
    "outputId": "c2218e98-aa34-4f23-ea14-eaf55e0e846d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.288873, Train accuracy: 0.194778, val accuracy: 0.206000\n",
      "Loss: 2.246671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234423, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230616, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229250, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228493, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228370, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228326, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228298, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228282, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228271, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228269, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228262, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228273, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228269, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228258, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228261, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZcUp8UJzIll"
   },
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86595,
     "status": "ok",
     "timestamp": 1594986943481,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "Vtfkh5mEzIln",
    "outputId": "021f7b41-48b9-4c27-dfc0-ec12c0ab982d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.332962, Train accuracy: 0.171444, val accuracy: 0.206000\n",
      "Loss: 2.331677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.330420, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.329192, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.327991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.326816, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.325668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.324544, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.323445, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322370, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.321317, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320288, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.319280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.318293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317328, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.316382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.315456, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314549, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313661, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.312791, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw2jTYQ2zIly"
   },
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JR0nqgMzIl0"
   },
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4249,
     "status": "ok",
     "timestamp": 1594986962454,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "uYDJy0xXzIl2",
    "outputId": "3c84744e-e503-4f19-dc92-867d07af45ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.338341, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: 2.319320, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.304046, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: 2.287941, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: 2.247924, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.158061, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.027167, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.872231, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.854597, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.788317, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.806797, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.767863, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.864404, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 1.748651, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.718247, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.727587, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.696710, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.736555, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.699044, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.690765, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.622640, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.633478, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.639127, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.620382, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.547987, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.535336, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.507429, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.487902, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.532658, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.490469, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.415299, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.356346, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.317230, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.335272, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.334480, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.272444, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.255644, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.235400, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.241111, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.179648, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.191789, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.187903, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.165061, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.161949, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.151497, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.185709, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.156200, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.151702, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.141633, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.162174, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.170669, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.147188, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.130211, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.117653, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.113089, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.108042, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.130126, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.129556, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.149688, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.128348, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.112325, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.112520, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.110040, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.096081, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.112691, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.106589, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.121086, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.108888, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.105819, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.116474, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.107749, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.098577, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.106582, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.092770, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.119393, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.134875, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.114135, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.117801, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.126257, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.111243, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.082120, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.088339, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.131337, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.095043, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.088557, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.088290, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.104793, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.100956, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.085776, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.091136, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.080003, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.080670, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.084393, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.104996, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.092788, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.085876, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.100861, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.109642, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.084070, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.087483, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.087338, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.083883, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.099376, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.103170, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.089276, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.078009, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.093918, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.079591, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.072883, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.087515, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.090719, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.074483, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.083921, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.077598, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.082733, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.104579, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.122460, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.077901, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.073774, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.094425, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.110929, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.082981, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.075733, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.092227, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.065285, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.098383, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.066367, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.069620, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.071936, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.072301, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.079079, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.093157, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.128172, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.095838, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.090273, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.089198, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.079686, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.066437, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.079546, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.059927, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.078695, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.069251, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.085852, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.083580, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.094165, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.072009, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.075959, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.076445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.061700, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.096665, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akvyqqB9zIl_"
   },
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1594988734707,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "tqVz3-a9zImB",
    "outputId": "9419bd19-1536-40df-89d5-b70467daec13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.307625, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.267564, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.144050, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.883211, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 2.005446, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.970075, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.864857, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.447577, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: 1.862212, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.760388, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.394584, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.776395, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 0.763300, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.355346, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.326102, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 2.188572, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.106838, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.336096, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.163743, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.067956, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=0.25, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eb7sUOawzImI"
   },
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Lx25Qqo4zImL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "301da287-0b63-420f-fe52-dde17dd56fb2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.235362, Train accuracy: 0.195889, val accuracy: 0.206000\n",
      "Loss: 2.230049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229651, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229355, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229138, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229352, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228483, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229221, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229056, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229170, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228932, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228802, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228882, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228150, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228521, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228227, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228598, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227648, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228524, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227362, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228392, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229027, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228380, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228647, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227373, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228571, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227836, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227647, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228005, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228244, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228379, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228058, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228189, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228688, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227935, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228257, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228045, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228112, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228170, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228527, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228869, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228213, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228278, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227979, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227945, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228235, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228019, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228168, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228211, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227537, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228010, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228110, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228216, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228587, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228128, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227669, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237609, Train accuracy: 0.195667, val accuracy: 0.206000\n",
      "Loss: 2.229399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230039, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229886, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228376, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229022, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229013, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228259, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229325, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228832, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228289, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228112, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228992, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227992, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228249, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228975, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229059, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229081, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228856, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228534, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228532, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228569, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228162, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227980, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228737, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228167, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228127, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228155, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228084, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228296, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228084, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228351, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227970, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227646, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229045, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228126, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227523, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228116, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227594, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227838, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228276, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228137, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228739, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228129, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228528, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227789, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228012, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227855, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227449, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228230, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227869, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228144, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228386, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227534, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228651, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228115, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227506, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227851, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238275, Train accuracy: 0.196000, val accuracy: 0.206000\n",
      "Loss: 2.229254, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229676, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229678, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228592, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229124, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228667, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228559, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228877, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228518, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228855, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228941, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227904, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227889, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228095, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228900, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228403, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228420, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228161, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228246, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227973, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228659, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228821, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228305, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228642, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228329, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228098, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228688, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228341, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228266, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228463, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228999, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227195, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227534, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228182, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227747, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228181, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228393, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227510, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227779, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228119, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228226, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228226, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228328, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228389, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228351, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228017, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227813, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227851, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227376, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227832, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228192, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228320, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227936, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227592, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228277, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227601, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227087, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228389, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228428, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228077, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228264, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238875, Train accuracy: 0.195889, val accuracy: 0.206000\n",
      "Loss: 2.230016, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229231, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229960, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228960, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229108, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229236, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228736, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228272, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228776, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228765, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228615, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228837, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228758, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228579, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228631, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228187, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228191, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227679, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227601, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228415, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228076, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227602, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227886, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227742, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228534, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228395, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228222, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228587, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227966, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228244, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227965, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227651, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228428, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228107, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226784, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227594, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228157, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228241, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228533, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228331, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228208, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228547, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227785, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228145, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227995, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227351, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228493, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227559, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227920, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228261, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228166, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228543, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228298, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228421, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227486, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227800, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227975, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228045, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228234, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228427, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227980, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236447, Train accuracy: 0.195889, val accuracy: 0.206000\n",
      "Loss: 2.230141, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230019, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229564, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229891, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229836, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230124, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229972, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229648, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228909, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229389, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229943, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229614, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229684, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229093, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229835, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229176, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229523, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230331, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230104, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229019, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229416, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229769, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229320, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229248, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229333, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229808, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229172, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229187, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229371, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228874, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229128, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228766, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228583, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229168, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229615, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228208, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229238, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229616, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228728, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228978, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228442, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228699, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229285, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228984, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228636, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228956, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228923, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229635, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228906, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229800, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228780, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228744, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229218, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229140, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228863, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238180, Train accuracy: 0.195667, val accuracy: 0.206000\n",
      "Loss: 2.229686, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229494, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229512, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230103, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229943, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229847, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230492, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229976, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229576, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229175, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229875, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229740, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229487, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229757, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229738, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229517, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230050, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229710, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230309, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229377, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229616, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228925, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229806, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228764, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229475, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229748, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229472, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229532, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230048, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229330, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229347, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229981, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229206, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229584, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228917, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229367, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229686, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228995, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228732, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229385, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229413, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229400, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229758, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229626, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229571, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229159, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229412, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229051, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229485, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229359, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228808, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229334, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229718, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228841, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228582, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229148, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228992, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228624, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228645, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229502, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238263, Train accuracy: 0.196000, val accuracy: 0.206000\n",
      "Loss: 2.231084, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229730, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229319, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229727, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230166, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229550, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229801, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229518, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229694, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230304, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230407, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230196, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229448, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229217, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229887, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229416, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229537, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229592, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230363, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229755, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229016, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229077, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229495, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229966, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229862, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229920, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229176, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229599, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229823, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229046, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229398, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229847, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229343, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229286, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229553, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229157, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229048, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229839, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229258, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229383, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229525, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229542, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229272, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228993, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229876, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228583, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229305, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229308, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228208, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229402, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229197, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229017, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229773, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228140, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228526, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238520, Train accuracy: 0.195444, val accuracy: 0.206000\n",
      "Loss: 2.229389, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230039, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230199, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229703, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229865, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229673, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231027, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230080, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229341, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229779, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230070, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229516, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229196, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229513, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229865, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229712, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230103, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229426, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229620, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229696, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229373, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229187, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229719, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229085, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228651, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228859, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229563, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229932, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229390, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229454, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229844, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230031, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228983, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229177, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229377, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229437, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228891, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229861, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228724, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229158, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229004, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230084, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229006, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230064, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229075, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229292, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229555, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229307, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229197, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228775, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229767, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228698, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229289, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228919, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229294, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229412, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228468, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229283, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236285, Train accuracy: 0.196222, val accuracy: 0.206000\n",
      "Loss: 2.230184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230700, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229873, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230345, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229453, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228953, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230654, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230079, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230202, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230017, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229481, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229577, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229771, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230548, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230919, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229564, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229706, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229344, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229275, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230278, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230257, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229682, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229490, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230027, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229840, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229160, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229979, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230768, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228716, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229835, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229679, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229431, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229647, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229161, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230246, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229577, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229169, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230070, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228827, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229615, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230249, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229857, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229301, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230394, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229919, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229566, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229961, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230128, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230099, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229496, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230200, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230128, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230672, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229428, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230013, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230537, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229521, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229905, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229389, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229325, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230533, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230331, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230015, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236848, Train accuracy: 0.194889, val accuracy: 0.206000\n",
      "Loss: 2.230179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229027, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229490, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230272, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229132, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230502, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229489, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230433, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229604, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230407, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229793, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229690, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229672, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229311, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229740, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230061, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229773, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229314, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229500, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229999, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229787, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230676, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230163, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229922, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230495, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230142, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229787, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229180, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230251, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230361, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229703, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230163, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230310, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230223, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230054, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230231, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229638, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229889, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229080, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230562, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229679, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230193, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229729, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229938, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230414, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230427, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230159, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229712, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230089, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229497, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229931, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230328, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229623, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230433, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230893, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230261, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230017, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230175, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230120, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228906, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237844, Train accuracy: 0.195556, val accuracy: 0.206000\n",
      "Loss: 2.229614, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229808, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230309, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229515, Train accuracy: 0.196222, val accuracy: 0.206000\n",
      "Loss: 2.229844, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229969, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228961, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229883, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229057, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229660, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230113, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230373, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229607, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230146, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230432, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229636, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230736, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230215, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229318, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230069, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229718, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229943, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230384, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230251, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229483, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231089, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230339, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229851, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229519, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229553, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229725, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229594, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229983, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229353, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230138, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229962, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230160, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230548, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229295, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230307, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230248, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230274, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229847, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230058, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229413, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229931, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229536, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230078, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229585, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229802, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229529, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229819, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229243, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230443, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230075, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230248, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229059, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230458, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229856, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230001, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229977, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229757, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240416, Train accuracy: 0.195556, val accuracy: 0.206000\n",
      "Loss: 2.230488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229048, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230068, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229920, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229892, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230863, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229901, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229353, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230516, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229732, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228969, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230813, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230542, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230973, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229434, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229244, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229604, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229865, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230184, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230418, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229959, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229721, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230193, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230201, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229212, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229624, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229477, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229409, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230383, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229167, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229593, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230428, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230171, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229329, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230205, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229686, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230066, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229896, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229919, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229229, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229965, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230237, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230014, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230180, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230219, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229463, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230363, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229398, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230436, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229319, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229768, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229580, Train accuracy: 0.196111, val accuracy: 0.206000\n",
      "Loss: 2.229518, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229597, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230080, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230129, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230165, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229412, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229886, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229459, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230175, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235687, Train accuracy: 0.195889, val accuracy: 0.206000\n",
      "Loss: 2.229320, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229254, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229950, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229071, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229480, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229648, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228401, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228585, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228948, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228430, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228754, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228401, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228750, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228220, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228548, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228107, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228368, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228706, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228173, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228653, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228264, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228715, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228272, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227450, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228437, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228406, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227470, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228239, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227759, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228165, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228307, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228663, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227306, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228147, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227645, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227801, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228624, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228021, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227571, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228691, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227672, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227769, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227645, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228326, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227496, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228055, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227981, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227496, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228233, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228283, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227968, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227443, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227538, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228327, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228157, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228464, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228433, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227734, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237340, Train accuracy: 0.195889, val accuracy: 0.206000\n",
      "Loss: 2.230216, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228833, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229294, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229655, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228555, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229278, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229040, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228885, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228258, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229212, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228815, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228809, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227731, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228763, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228289, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228476, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228213, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228597, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228318, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228785, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227729, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228824, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227670, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227942, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227681, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228553, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227941, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229155, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228003, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228824, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228135, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227895, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227790, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227801, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227691, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228507, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228395, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228099, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228276, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228246, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228318, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228342, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228845, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228340, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228126, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227859, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227967, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227838, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227737, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228156, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227603, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228287, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228069, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228001, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228431, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238343, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230029, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229644, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229252, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229425, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228927, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228219, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228170, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228861, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229079, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228726, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229007, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228596, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228129, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228216, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228394, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228334, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228114, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227957, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227824, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228490, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227664, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228174, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227906, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228221, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228014, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227759, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227803, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228169, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228076, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228573, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228816, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227771, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228101, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227827, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227631, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228345, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228116, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227731, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228314, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228375, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228277, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227754, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227678, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227874, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b38db2039e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                   learning_rate_decay=lrd)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Training/ML\\DS/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Training/ML\\DS/dlcourse_ai/assignments/assignment2/optim.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, w, d_w, learning_rate)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvelocity\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [0.2, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strength = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "learning_rate_decay = [0.9, 0.99, 1]\n",
    "hidden_layer_size = np.arange(50, 250, 50)\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strength:\n",
    "        for lrd in learning_rate_decay:\n",
    "            for hls in hidden_layer_size:\n",
    "                model = TwoLayerNet(n_input=train_X.shape[1],\n",
    "                                    n_output=10,\n",
    "                                    hidden_layer_size=hls,\n",
    "                                    reg=rs)\n",
    "                dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "                trainer = Trainer(model, dataset, MomentumSGD(),\n",
    "                                  learning_rate=lr,\n",
    "                                  num_epochs=ne,\n",
    "                                  batch_size=bs,\n",
    "                                  learning_rate_decay=lrd)\n",
    "\n",
    "                loss_history, train_history, val_history = trainer.fit()\n",
    "                history.append((lr, rs, lrd, hls, ne, bs, np.max(val_history)))\n",
    "\n",
    "print(sorted(history, key=lambda x: x[6]))\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.237767, Train accuracy: 0.195667, val accuracy: 0.206000\n",
      "Loss: 2.220303, Train accuracy: 0.203889, val accuracy: 0.218000\n",
      "Loss: 2.203015, Train accuracy: 0.215000, val accuracy: 0.226000\n",
      "Loss: 2.188670, Train accuracy: 0.227111, val accuracy: 0.215000\n",
      "Loss: 2.177285, Train accuracy: 0.229778, val accuracy: 0.228000\n",
      "Loss: 2.169441, Train accuracy: 0.237778, val accuracy: 0.227000\n",
      "Loss: 2.160665, Train accuracy: 0.241556, val accuracy: 0.220000\n",
      "Loss: 2.156481, Train accuracy: 0.246000, val accuracy: 0.232000\n",
      "Loss: 2.150515, Train accuracy: 0.252444, val accuracy: 0.214000\n",
      "Loss: 2.143886, Train accuracy: 0.253111, val accuracy: 0.211000\n",
      "Loss: 2.135919, Train accuracy: 0.258222, val accuracy: 0.246000\n",
      "Loss: 2.134015, Train accuracy: 0.265000, val accuracy: 0.236000\n",
      "Loss: 2.127473, Train accuracy: 0.268889, val accuracy: 0.240000\n",
      "Loss: 2.125535, Train accuracy: 0.265889, val accuracy: 0.216000\n",
      "Loss: 2.124307, Train accuracy: 0.271111, val accuracy: 0.229000\n",
      "Loss: 2.116750, Train accuracy: 0.274889, val accuracy: 0.225000\n",
      "Loss: 2.115312, Train accuracy: 0.274000, val accuracy: 0.238000\n",
      "Loss: 2.109789, Train accuracy: 0.278333, val accuracy: 0.215000\n",
      "Loss: 2.108810, Train accuracy: 0.276889, val accuracy: 0.224000\n",
      "Loss: 2.110442, Train accuracy: 0.279333, val accuracy: 0.240000\n",
      "Loss: 2.100678, Train accuracy: 0.283333, val accuracy: 0.233000\n",
      "Loss: 2.100909, Train accuracy: 0.280111, val accuracy: 0.244000\n",
      "Loss: 2.097433, Train accuracy: 0.284222, val accuracy: 0.231000\n",
      "Loss: 2.096254, Train accuracy: 0.286111, val accuracy: 0.249000\n",
      "Loss: 2.092324, Train accuracy: 0.287222, val accuracy: 0.232000\n",
      "Loss: 2.091095, Train accuracy: 0.288000, val accuracy: 0.262000\n",
      "Loss: 2.088315, Train accuracy: 0.289889, val accuracy: 0.236000\n",
      "Loss: 2.086003, Train accuracy: 0.289111, val accuracy: 0.248000\n",
      "Loss: 2.086363, Train accuracy: 0.289000, val accuracy: 0.214000\n",
      "Loss: 2.084071, Train accuracy: 0.291111, val accuracy: 0.250000\n",
      "Loss: 2.077889, Train accuracy: 0.291667, val accuracy: 0.241000\n",
      "Loss: 2.078259, Train accuracy: 0.291889, val accuracy: 0.208000\n",
      "Loss: 2.072154, Train accuracy: 0.297778, val accuracy: 0.250000\n",
      "Loss: 2.076160, Train accuracy: 0.296000, val accuracy: 0.242000\n",
      "Loss: 2.069167, Train accuracy: 0.298111, val accuracy: 0.239000\n",
      "Loss: 2.067869, Train accuracy: 0.298111, val accuracy: 0.241000\n",
      "Loss: 2.064751, Train accuracy: 0.298556, val accuracy: 0.218000\n",
      "Loss: 2.064754, Train accuracy: 0.298111, val accuracy: 0.243000\n",
      "Loss: 2.065107, Train accuracy: 0.299556, val accuracy: 0.253000\n",
      "Loss: 2.063492, Train accuracy: 0.301556, val accuracy: 0.242000\n",
      "Loss: 2.057652, Train accuracy: 0.299667, val accuracy: 0.239000\n",
      "Loss: 2.056733, Train accuracy: 0.302667, val accuracy: 0.260000\n",
      "Loss: 2.057766, Train accuracy: 0.302778, val accuracy: 0.258000\n",
      "Loss: 2.056056, Train accuracy: 0.304000, val accuracy: 0.246000\n",
      "Loss: 2.054373, Train accuracy: 0.302667, val accuracy: 0.235000\n",
      "Loss: 2.049143, Train accuracy: 0.305000, val accuracy: 0.231000\n",
      "Loss: 2.049079, Train accuracy: 0.305778, val accuracy: 0.244000\n",
      "Loss: 2.049851, Train accuracy: 0.304000, val accuracy: 0.242000\n",
      "Loss: 2.045449, Train accuracy: 0.303778, val accuracy: 0.230000\n",
      "Loss: 2.047150, Train accuracy: 0.305889, val accuracy: 0.240000\n",
      "Loss: 2.047934, Train accuracy: 0.305667, val accuracy: 0.246000\n",
      "Loss: 2.043139, Train accuracy: 0.306111, val accuracy: 0.243000\n",
      "Loss: 2.041424, Train accuracy: 0.308556, val accuracy: 0.234000\n",
      "Loss: 2.039051, Train accuracy: 0.310333, val accuracy: 0.237000\n",
      "Loss: 2.036435, Train accuracy: 0.312444, val accuracy: 0.248000\n",
      "Loss: 2.037502, Train accuracy: 0.309000, val accuracy: 0.248000\n",
      "Loss: 2.032366, Train accuracy: 0.307111, val accuracy: 0.239000\n",
      "Loss: 2.032055, Train accuracy: 0.312889, val accuracy: 0.244000\n",
      "Loss: 2.032171, Train accuracy: 0.311222, val accuracy: 0.225000\n",
      "Loss: 2.027852, Train accuracy: 0.317111, val accuracy: 0.239000\n",
      "Loss: 2.032696, Train accuracy: 0.309556, val accuracy: 0.240000\n",
      "Loss: 2.030715, Train accuracy: 0.309111, val accuracy: 0.238000\n",
      "Loss: 2.027543, Train accuracy: 0.307222, val accuracy: 0.232000\n",
      "Loss: 2.029732, Train accuracy: 0.312778, val accuracy: 0.245000\n",
      "Loss: 2.019014, Train accuracy: 0.314333, val accuracy: 0.229000\n",
      "Loss: 2.021130, Train accuracy: 0.310222, val accuracy: 0.233000\n",
      "Loss: 2.021213, Train accuracy: 0.315667, val accuracy: 0.238000\n",
      "Loss: 2.018334, Train accuracy: 0.321444, val accuracy: 0.227000\n",
      "Loss: 2.017659, Train accuracy: 0.316222, val accuracy: 0.235000\n",
      "Loss: 2.021997, Train accuracy: 0.314333, val accuracy: 0.253000\n",
      "Loss: 2.017004, Train accuracy: 0.313222, val accuracy: 0.241000\n",
      "Loss: 2.017148, Train accuracy: 0.317111, val accuracy: 0.231000\n",
      "Loss: 2.013133, Train accuracy: 0.316333, val accuracy: 0.238000\n",
      "Loss: 2.012689, Train accuracy: 0.319667, val accuracy: 0.254000\n",
      "Loss: 2.011742, Train accuracy: 0.319889, val accuracy: 0.243000\n",
      "Loss: 2.010036, Train accuracy: 0.317667, val accuracy: 0.237000\n",
      "Loss: 2.008140, Train accuracy: 0.317333, val accuracy: 0.246000\n",
      "Loss: 2.005103, Train accuracy: 0.323667, val accuracy: 0.231000\n",
      "Loss: 2.005133, Train accuracy: 0.323000, val accuracy: 0.243000\n",
      "Loss: 2.002152, Train accuracy: 0.319111, val accuracy: 0.244000\n",
      "Loss: 2.002770, Train accuracy: 0.325000, val accuracy: 0.247000\n",
      "Loss: 2.005705, Train accuracy: 0.320778, val accuracy: 0.253000\n",
      "Loss: 2.001141, Train accuracy: 0.322444, val accuracy: 0.230000\n",
      "Loss: 2.001335, Train accuracy: 0.324333, val accuracy: 0.236000\n",
      "Loss: 1.997321, Train accuracy: 0.320778, val accuracy: 0.229000\n",
      "Loss: 2.001861, Train accuracy: 0.324667, val accuracy: 0.251000\n",
      "Loss: 1.997852, Train accuracy: 0.323778, val accuracy: 0.237000\n",
      "Loss: 1.994643, Train accuracy: 0.323222, val accuracy: 0.251000\n",
      "Loss: 1.993638, Train accuracy: 0.323333, val accuracy: 0.223000\n",
      "Loss: 1.990996, Train accuracy: 0.325444, val accuracy: 0.249000\n",
      "Loss: 1.993134, Train accuracy: 0.329667, val accuracy: 0.225000\n",
      "Loss: 1.991026, Train accuracy: 0.322889, val accuracy: 0.233000\n",
      "Loss: 1.991756, Train accuracy: 0.323111, val accuracy: 0.257000\n",
      "Loss: 1.989547, Train accuracy: 0.324111, val accuracy: 0.243000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-64c262605597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   learning_rate_decay=0.99)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/Training/ML\\DS/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Training/ML\\DS/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0minp_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mout_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# After that, implement l2 regularization on all params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Training/ML\\DS/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# the previous assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0md_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input=train_X.shape[1],\n",
    "                    n_output=10,\n",
    "                    hidden_layer_size=1000,\n",
    "                    reg=0)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(),\n",
    "                  learning_rate=0.25,\n",
    "                  num_epochs=100,\n",
    "                  batch_size=64,\n",
    "                  learning_rate_decay=0.99)\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1594987695419,
     "user": {
      "displayName": "Павел Ананьев",
      "photoUrl": "",
      "userId": "07746041064703407151"
     },
     "user_tz": 0
    },
    "id": "CLlN3zVbzIml",
    "outputId": "14c10efc-0225-469b-c049-52bfb5e6a840"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f49c369d3c8>]"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz8qGPo4zImt"
   },
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8S7nnjvzImu"
   },
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
