{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/snake/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/snake/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADJ', 'PRT', 'NUM', 'X', 'PRON', 'ADV', 'NOUN', 'ADP', 'DET', '.', 'CONJ', 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind for ind, word in enumerate(words, start=1)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind for ind, tag in enumerate(tags, start=1)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(len(word2ind), 100)\n",
    "a(X_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.lstm_layers_count = lstm_layers_count\n",
    "        \n",
    "        self.embedings = nn.Embedding(self.vocab_size, self.word_emb_dim)\n",
    "        self.LSTM = nn.LSTM(self.word_emb_dim, self.lstm_hidden_dim, self.lstm_layers_count)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedings(inputs)\n",
    "        output = self.LSTM(embedded)\n",
    "        return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected target size (32, 128), got torch.Size([32, 4])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-59ddabf993df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 2228\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   2229\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2230\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (32, 128), got torch.Size([32, 4])"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion(logits, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = <calc loss>\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = <calc accuracy>\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create me>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <use me>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [],
   "source": [
    "<calc test accuracy>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
